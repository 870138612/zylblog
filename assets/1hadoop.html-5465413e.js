const e=JSON.parse('{"key":"v-520e4fc2","path":"/note/aother/1hadoop.html","title":"Hadoop知识点总结","lang":"zh-CN","frontmatter":{"title":"Hadoop知识点总结","icon":"biji1","date":"2023-09-19T00:00:00.000Z","star":1,"category":["笔记"],"tag":["大数据","Hadoop"]},"headers":[{"level":3,"title":"MapperReduce 详细的工作流程","slug":"mapperreduce-详细的工作流程","link":"#mapperreduce-详细的工作流程","children":[]},{"level":3,"title":"HDFS 分布式存储工作机制","slug":"hdfs-分布式存储工作机制","link":"#hdfs-分布式存储工作机制","children":[]},{"level":3,"title":"yarn 资源调度工作机制","slug":"yarn-资源调度工作机制","link":"#yarn-资源调度工作机制","children":[]},{"level":3,"title":"Hadoop 的组成","slug":"hadoop-的组成","link":"#hadoop-的组成","children":[]},{"level":3,"title":"MapperReduce 的 Shuffle 过程","slug":"mapperreduce-的-shuffle-过程","link":"#mapperreduce-的-shuffle-过程","children":[]},{"level":3,"title":"Hadoop 的垃圾回收机制","slug":"hadoop-的垃圾回收机制","link":"#hadoop-的垃圾回收机制","children":[]},{"level":3,"title":"","slug":"","link":"#","children":[]}],"git":{"createdTime":1695112865000,"updatedTime":1695174687000,"contributors":[{"name":"ZYL1210","email":"870138612@qq.com","commits":3}]},"readingTime":{"minutes":5.05,"words":1515},"filePathRelative":"note/aother/1hadoop.md","localizedDate":"2023年9月19日","excerpt":"<h3> MapperReduce 详细的工作流程</h3>\\n<ol>\\n<li>在客户端执行 submit() 方法之前，先会获取待读取文件的信息。</li>\\n<li>将 job 提交给 yarn，这时候会带上三个信息过去。（文件的切片信息，jar，job.xml)</li>\\n<li>yarn 会通过切片信息去计算需要启动的 maptask 数量，然后启动 maptask。</li>\\n<li>maptask 会调用 InPutFormat() 方法去 HDFS 上面读取文件，InputFormat() 会再去调用 RecordRead() 方法将数据以行首字母的偏移量作为 key，一行数据作为 value 传给 mapper() 方法。</li>\\n<li>mapper 方法做完处理之后，将数据转移到分区方法中，对数据进行标注之后，发送到环形缓冲区。</li>\\n<li>环形缓冲区的大小默认是 100M，达到 80% 将会发生溢写。</li>\\n<li>在溢写之前会做一个排序动作，排序的规则是按照 key 进行字典排序，排序的手段是快速排序。</li>\\n<li>溢写会产生出大量的溢写文件，会再次调用 merge() 方法，使用归并排序，默认十个溢写文件构成一个大文件。</li>\\n<li>也可以对溢写文件进行一个 localReduce，也就是 combiner 的操作，但前提是 combiner 的结果不能对最终结果产生影响。</li>\\n<li>等待所有的 maptask 执行完毕之后，会启动一定数量的 reducetask。</li>\\n<li>reducetask 会在 map 端拉取数据，数据会先加载到内存中，内存不够会写入磁盘，等待所有的数据拉去完毕之后，将这些数据再次进行一次归并操作。</li>\\n<li>归并之后的文件会再进行一次分组操作，然后将数据以组为单位发送给 reduce() 方法。</li>\\n<li>reduce() 方法会做一些逻辑判断，最终调用 OutputFormat() 方法，OutputFormat() 会调用 RecordWrite() 方法将数据以 KV 的形式写出到 HDFS 上。</li>\\n</ol>"}');export{e as data};
